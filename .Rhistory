profit.svm1 <- cumsum(14.5*c.valid[order(post.valid.svm1, decreasing=T)]-2)
plot(profit.svm1) # see how profits change as more mailings are made
n.mail.valid <- which.max(profit.svm1) # number of mailings that maximizes profits
c(n.mail.valid, max(profit.svm1)) # report number of mailings and maximum profit
cutoff.svm1 <- sort(post.valid.svm1, decreasing=T)[n.mail.valid+1] # set cutoff based on n.mail.valid
chat.valid.svm1 <- ifelse(post.valid.svm1>cutoff.svm1, 1, 0) # mail to everyone above the cutoff
table(chat.valid.svm1, c.valid) # classification table
## computing a simple ROC curve (x-axis: fpr, y-axis: tpr)
library(ROCR)
pred <- prediction(chat.valid.svm1, c.valid)
perf <- performance(pred,"tpr","fpr")
auc.perf = performance(pred, measure = "auc")
auc.perf@y.values
plot(perf)
set.seed(1)
model.svm1 =  svm(donr ~ reg1 + reg2 + home + chld + hinc + inca + tgif + tlag + wrat2 +
wrat3 + hinc2 + ln_incm + ln_tgif + ln_lgif + ln_tdon + ln_tlag +
ln_agif + sr_chld + sr_tdon + sr_tlag,
data=data.train.std.c, kernel="radial", cost=0.1, scale=FALSE)
post.valid.svm1 <- predict(model.svm1, data.valid.std.c) # n.valid post probs
profit.svm1 <- cumsum(14.5*c.valid[order(post.valid.svm1, decreasing=T)]-2)
plot(profit.svm1) # see how profits change as more mailings are made
n.mail.valid <- which.max(profit.svm1) # number of mailings that maximizes profits
c(n.mail.valid, max(profit.svm1)) # report number of mailings and maximum profit
cutoff.svm1 <- sort(post.valid.svm1, decreasing=T)[n.mail.valid+1] # set cutoff based on n.mail.valid
chat.valid.svm1 <- ifelse(post.valid.svm1>cutoff.svm1, 1, 0) # mail to everyone above the cutoff
table(chat.valid.svm1, c.valid) # classification table
## computing a simple ROC curve (x-axis: fpr, y-axis: tpr)
library(ROCR)
pred <- prediction(chat.valid.svm1, c.valid)
perf <- performance(pred,"tpr","fpr")
auc.perf = performance(pred, measure = "auc")
auc.perf@y.values
plot(perf)
set.seed(1)
model.svm1 = svm(donr ~ reg1 + reg2 + home + sr_chld + hinc + hinc2 + wrat2 + wrat3 +
ln_incm + ln_tgif + sr_tdon + sr_tlag,
data=data.train.std.c, kernel="radial", cost=0.1, scale=FALSE)
post.valid.svm1 <- predict(model.svm1, data.valid.std.c) # n.valid post probs
profit.svm1 <- cumsum(14.5*c.valid[order(post.valid.svm1, decreasing=T)]-2)
plot(profit.svm1) # see how profits change as more mailings are made
n.mail.valid <- which.max(profit.svm1) # number of mailings that maximizes profits
c(n.mail.valid, max(profit.svm1)) # report number of mailings and maximum profit
cutoff.svm1 <- sort(post.valid.svm1, decreasing=T)[n.mail.valid+1] # set cutoff based on n.mail.valid
chat.valid.svm1 <- ifelse(post.valid.svm1>cutoff.svm1, 1, 0) # mail to everyone above the cutoff
table(chat.valid.svm1, c.valid) # classification table
## computing a simple ROC curve (x-axis: fpr, y-axis: tpr)
library(ROCR)
pred <- prediction(chat.valid.svm1, c.valid)
perf <- performance(pred,"tpr","fpr")
auc.perf = performance(pred, measure = "auc")
auc.perf@y.values
plot(perf)
# Colormap for classification models
library(gplots)
library(RColorBrewer)
d1 = read.csv("classmap2.csv", row.names = 1, header=T)
d2 = read.csv("classactual.csv", row.names = 1, header=T)
rownames = rownames(d1)
#d1 = d1[,-c(16,17)]
#d2 = d2[,-c(16,17)]
d1 = as.matrix(d1)
d2 = as.matrix(d2)
# creates a own color palette from blue to red
my_palette <- colorRampPalette(c("blue","yellow","red"))(n = 299)
# (optional) defines the color breaks manually for a "skewed" color transition
col_breaks = c(seq(0,0.799,length=100),   # for blue
seq(0.80,0.929,length=100),   # for yellow
seq(0.93,1.0,length=100))   # for red
# creates a 5 x 5 inch image
png("classheatmap.png",     # create PNG for the heat map
width = 10*300,        # 5 x 300 pixels
height = 13*300,
res = 300,            # 300 pixels per inch
pointsize = 10)        # smaller font size
heatmap.2(d1,
cellnote = d2,        # same data set for cell labels
main = "Classification Metrics", # heat map title
notecol="black",      # change font color of cell labels to black
density.info="none",  # turns off density plot inside color legend
trace="none",         # turns off trace lines inside the heat map
margins =c(12,10),     # widens margins around plot
col=my_palette,       # use on color palette defined earlier
breaks=col_breaks,    # enable color transition at specified limits
dendrogram="none",    # only draw a row dendrogram
Colv=FALSE,           # turn off column clustering
Rowv=FALSE,           # turn off row clustering
keysize=25,
lmat=rbind(c(2),c(3),c(1),c(4)),
lhei=c(5,5,18,0),
lwid=c(1))
dev.off()
# Colormap for classification models
library(gplots)
library(RColorBrewer)
d1 = read.csv("classmap2.csv", row.names = 1, header=T)
d2 = read.csv("classactual2.csv", row.names = 1, header=T)
rownames = rownames(d1)
#d1 = d1[,-c(16,17)]
#d2 = d2[,-c(16,17)]
d1 = as.matrix(d1)
d2 = as.matrix(d2)
# creates a own color palette from blue to red
my_palette <- colorRampPalette(c("blue","yellow","red"))(n = 299)
# (optional) defines the color breaks manually for a "skewed" color transition
col_breaks = c(seq(0,0.799,length=100),   # for blue
seq(0.80,0.929,length=100),   # for yellow
seq(0.93,1.0,length=100))   # for red
# creates a 5 x 5 inch image
png("classheatmap.png",     # create PNG for the heat map
width = 10*300,        # 5 x 300 pixels
height = 13*300,
res = 300,            # 300 pixels per inch
pointsize = 10)        # smaller font size
heatmap.2(d1,
cellnote = d2,        # same data set for cell labels
main = "Classification Metrics", # heat map title
notecol="black",      # change font color of cell labels to black
density.info="none",  # turns off density plot inside color legend
trace="none",         # turns off trace lines inside the heat map
margins =c(12,10),     # widens margins around plot
col=my_palette,       # use on color palette defined earlier
breaks=col_breaks,    # enable color transition at specified limits
dendrogram="none",    # only draw a row dendrogram
Colv=FALSE,           # turn off column clustering
Rowv=FALSE,           # turn off row clustering
keysize=25,
lmat=rbind(c(2),c(3),c(1),c(4)),
lhei=c(5,5,18,0),
lwid=c(1))
dev.off()
# Colormap for classification models
library(gplots)
library(RColorBrewer)
d1 = read.csv("classmap2.csv", row.names = 1, header=T)
d2 = read.csv("classactual2.csv", row.names = 1, header=T)
rownames = rownames(d1)
#d1 = d1[,-c(16,17)]
#d2 = d2[,-c(16,17)]
d1 = as.matrix(d1)
d2 = as.matrix(d2)
# creates a own color palette from blue to red
my_palette <- colorRampPalette(c("blue","yellow","red"))(n = 299)
# (optional) defines the color breaks manually for a "skewed" color transition
col_breaks = c(seq(0,0.799,length=100),   # for blue
seq(0.80,0.929,length=100),   # for yellow
seq(0.93,1.0,length=100))   # for red
# creates a 5 x 5 inch image
png("classheatmap.png",     # create PNG for the heat map
width = 10*300,        # 5 x 300 pixels
height = 13*300,
res = 300,            # 300 pixels per inch
pointsize = 10)        # smaller font size
heatmap.2(d1,
cellnote = d2,        # same data set for cell labels
main = "Classification Metrics", # heat map title
notecol="black",      # change font color of cell labels to black
density.info="none",  # turns off density plot inside color legend
trace="none",         # turns off trace lines inside the heat map
margins =c(12,10),     # widens margins around plot
col=my_palette,       # use on color palette defined earlier
breaks=col_breaks,    # enable color transition at specified limits
dendrogram="none",    # only draw a row dendrogram
Colv=FALSE,           # turn off column clustering
Rowv=FALSE,           # turn off row clustering
keysize=25,
lmat=rbind(c(2),c(3),c(1),c(4)),
lhei=c(5,5,18,0),
lwid=c(1))
dev.off()
# Colormap for classification models
library(gplots)
library(RColorBrewer)
d1 = read.csv("classmap2.csv", row.names = 1, header=T)
d2 = read.csv("classactual2.csv", row.names = 1, header=T)
rownames = rownames(d1)
#d1 = d1[,-c(16,17)]
#d2 = d2[,-c(16,17)]
d1 = as.matrix(d1)
d2 = as.matrix(d2)
# creates a own color palette from blue to red
my_palette <- colorRampPalette(c("blue","yellow","red"))(n = 299)
# (optional) defines the color breaks manually for a "skewed" color transition
col_breaks = c(seq(0,0.799,length=100),   # for blue
seq(0.80,0.929,length=100),   # for yellow
seq(0.93,1.0,length=100))   # for red
# creates a 5 x 5 inch image
png("classheatmap.png",     # create PNG for the heat map
width = 10*300,        # 5 x 300 pixels
height = 13*300,
res = 300,            # 300 pixels per inch
pointsize = 10)        # smaller font size
heatmap.2(d1,
cellnote = d2,        # same data set for cell labels
main = "Classification Metrics", # heat map title
notecol="black",      # change font color of cell labels to black
density.info="none",  # turns off density plot inside color legend
trace="none",         # turns off trace lines inside the heat map
margins =c(12,10),     # widens margins around plot
col=my_palette,       # use on color palette defined earlier
breaks=col_breaks,    # enable color transition at specified limits
dendrogram="none",    # only draw a row dendrogram
Colv=FALSE,           # turn off column clustering
Rowv=FALSE,           # turn off row clustering
keysize=25,
lmat=rbind(c(2),c(3),c(1),c(4)),
lhei=c(5,5,18,0),
lwid=c(1))
dev.off()
# Colormap for classification models
library(gplots)
library(RColorBrewer)
d1 = read.csv("classmap2.csv", row.names = 1, header=T)
d2 = read.csv("classactual2.csv", row.names = 1, header=T)
rownames = rownames(d1)
#d1 = d1[,-c(16,17)]
#d2 = d2[,-c(16,17)]
d1 = as.matrix(d1)
d2 = as.matrix(d2)
# creates a own color palette from blue to red
my_palette <- colorRampPalette(c("blue","yellow","red"))(n = 299)
# (optional) defines the color breaks manually for a "skewed" color transition
col_breaks = c(seq(0,0.799,length=100),   # for blue
seq(0.80,0.929,length=100),   # for yellow
seq(0.93,1.0,length=100))   # for red
# creates a 5 x 5 inch image
png("classheatmap.png",     # create PNG for the heat map
width = 10*300,        # 5 x 300 pixels
height = 13*300,
res = 300,            # 300 pixels per inch
pointsize = 10)        # smaller font size
heatmap.2(d1,
cellnote = d2,        # same data set for cell labels
main = "Classification Metrics", # heat map title
notecol="black",      # change font color of cell labels to black
density.info="none",  # turns off density plot inside color legend
trace="none",         # turns off trace lines inside the heat map
margins =c(10,8),     # widens margins around plot
col=my_palette,       # use on color palette defined earlier
breaks=col_breaks,    # enable color transition at specified limits
dendrogram="none",    # only draw a row dendrogram
Colv=FALSE,           # turn off column clustering
Rowv=FALSE,           # turn off row clustering
keysize=25,
lmat=rbind(c(2),c(3),c(1),c(4)),
lhei=c(5,5,18,0),
lwid=c(1))
dev.off()
# Colormap for classification models
library(gplots)
library(RColorBrewer)
d1 = read.csv("classmap2.csv", row.names = 1, header=T)
d2 = read.csv("classactual2.csv", row.names = 1, header=T)
rownames = rownames(d1)
#d1 = d1[,-c(16,17)]
#d2 = d2[,-c(16,17)]
d1 = as.matrix(d1)
d2 = as.matrix(d2)
# creates a own color palette from blue to red
my_palette <- colorRampPalette(c("blue","yellow","red"))(n = 299)
# (optional) defines the color breaks manually for a "skewed" color transition
col_breaks = c(seq(0,0.799,length=100),   # for blue
seq(0.80,0.929,length=100),   # for yellow
seq(0.93,1.0,length=100))   # for red
# creates a 5 x 5 inch image
png("classheatmap.png",     # create PNG for the heat map
width = 11*300,        # 5 x 300 pixels
height = 15*300,
res = 300,            # 300 pixels per inch
pointsize = 10)        # smaller font size
heatmap.2(d1,
cellnote = d2,        # same data set for cell labels
main = "Classification Metrics", # heat map title
notecol="black",      # change font color of cell labels to black
density.info="none",  # turns off density plot inside color legend
trace="none",         # turns off trace lines inside the heat map
margins =c(10,8),     # widens margins around plot
col=my_palette,       # use on color palette defined earlier
breaks=col_breaks,    # enable color transition at specified limits
dendrogram="none",    # only draw a row dendrogram
Colv=FALSE,           # turn off column clustering
Rowv=FALSE,           # turn off row clustering
keysize=25,
lmat=rbind(c(2),c(3),c(1),c(4)),
lhei=c(5,5,18,0),
lwid=c(1))
dev.off()
# Colormap for classification models
library(gplots)
library(RColorBrewer)
d1 = read.csv("classmap2.csv", row.names = 1, header=T)
d2 = read.csv("classactual2.csv", row.names = 1, header=T)
rownames = rownames(d1)
#d1 = d1[,-c(16,17)]
#d2 = d2[,-c(16,17)]
d1 = as.matrix(d1)
d2 = as.matrix(d2)
# creates a own color palette from blue to red
my_palette <- colorRampPalette(c("blue","yellow","red"))(n = 299)
# (optional) defines the color breaks manually for a "skewed" color transition
col_breaks = c(seq(0,0.799,length=100),   # for blue
seq(0.80,0.929,length=100),   # for yellow
seq(0.93,1.0,length=100))   # for red
# creates a 5 x 5 inch image
png("classheatmap.png",     # create PNG for the heat map
width = 11*300,        # 5 x 300 pixels
height = 17*300,
res = 300,            # 300 pixels per inch
pointsize = 10)        # smaller font size
heatmap.2(d1,
cellnote = d2,        # same data set for cell labels
main = "Classification Metrics", # heat map title
notecol="black",      # change font color of cell labels to black
density.info="none",  # turns off density plot inside color legend
trace="none",         # turns off trace lines inside the heat map
margins =c(10,8),     # widens margins around plot
col=my_palette,       # use on color palette defined earlier
breaks=col_breaks,    # enable color transition at specified limits
dendrogram="none",    # only draw a row dendrogram
Colv=FALSE,           # turn off column clustering
Rowv=FALSE,           # turn off row clustering
keysize=25,
lmat=rbind(c(2),c(3),c(1),c(4)),
lhei=c(5,5,18,0),
lwid=c(1))
dev.off()
# Set up data for analysis (initial steps)
data.train <- charity[charity$part=="train",]
x.train <- data.train[,c(2:21, 25:55)]
c.train <- data.train[,22] # donr
n.train.c <- length(c.train) # 3984
y.train <- data.train[c.train==1,23] # damt for observations with donr=1
n.train.y <- length(y.train) # 1995
data.valid <- charity[charity$part=="valid",]
x.valid <- data.valid[,c(2:21, 25:55)]
c.valid <- data.valid[,22] # donr
n.valid.c <- length(c.valid) # 2018
y.valid <- data.valid[c.valid==1,23] # damt for observations with donr=1
n.valid.y <- length(y.valid) # 999
data.test <- charity[charity$part=="test",]
n.test <- dim(data.test)[1] # 2007
x.test <- data.test[,c(2:21, 25:55)]
# Set up data for analysis (data splitting before standardization)
rm(data.train.std.y, data.train.std.c, data.valid.std.y, data.valid.std.c, data.test.std)
x.train.mean <- apply(x.train, 2, mean)
x.train.sd <- apply(x.train, 2, sd)
x.train.std <- t((t(x.train)-x.train.mean)/x.train.sd) # standardize to have zero mean and unit sd
apply(x.train.std, 2, mean) # check zero mean
apply(x.train.std, 2, sd) # check unit sd
data.train.std.c <- data.frame(x.train.std, donr=c.train) # to classify donr
data.train.std.y <- data.frame(x.train.std[c.train==1,], damt=y.train) # to predict damt when donr=1
x.valid.std <- t((t(x.valid)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.valid.std.c <- data.frame(x.valid.std, donr=c.valid) # to classify donr
data.valid.std.y <- data.frame(x.valid.std[c.valid==1,], damt=y.valid) # to predict damt when donr=1
x.test.std <- t((t(x.test)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.test.std <- data.frame(x.test.std)
# Artificial Neural Network
library(neuralnet)
xc_vars = c("reg1","reg2","home","sr_chld","hinc","hinc2","wrat2","wrat3",
"ln_incm","ln_tgif","sr_tdon","sr_tlag")
xb_vars = c("reg1","reg2","home","chld","hinc","inca","tgif","tlag","wrat2",
"wrat3","hinc2","ln_incm","ln_tgif","ln_lgif","ln_tdon","ln_tlag",
"ln_agif","sr_chld","sr_tdon","sr_tlag")
xa_vars = c("reg1","reg2","reg3","reg4","home","chld","hinc","hinc2","genf","wrat",
"ln_avhv","incm","inca","plow","npro","tgif","lgif","rgif","tdon",
"tlag","agif")
xc1 = data.train.std.c[,xc_vars]
xb1 = data.train.std.c[,xb_vars]
xa1 = data.train.std.c[,xa_vars]
xvc1 = data.valid.std.c[,xc_vars]
xvb1 = data.valid.std.c[,xb_vars]
xva1 = data.valid.std.c[,xa_vars]
# M12a: Default Original subset
set.seed(1)
model.nn1 <-  neuralnet(donr ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + hinc2 + genf + wrat +
ln_avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif,
data=data.train.std.c, hidden=10, threshold=0.01)
# M12a: Default Original subset
set.seed(1)
model.nn1 <-  neuralnet(donr ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + hinc2 + genf + wrat +
ln_avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif,
data=data.train.std.c, hidden=4, threshold=0.01)
newdata=xva1
post.valid.nn1 <- compute(model.nn1, newdata) # n.valid post probs
profit.nn1 <- cumsum(14.5*c.valid[order(post.valid.nn1, decreasing=T)]-2)
plot(profit.nn1) # see how profits change as more mailings are made
n.mail.valid <- which.max(profit.nn1) # number of mailings that maximizes profits
c(n.mail.valid, max(profit.nn1)) # report number of mailings and maximum profit
cutoff.nn1 <- sort(post.valid.nn1, decreasing=T)[n.mail.valid+1] # set cutoff based on n.mail.valid
chat.valid.nn1 <- ifelse(post.valid.nn1>cutoff.nn1, 1, 0) # mail to everyone above the cutoff
table(chat.valid.nn1, c.valid) # classification table
## computing a simple ROC curve (x-axis: fpr, y-axis: tpr)
library(ROCR)
pred <- prediction(chat.valid.nn1, c.valid)
perf <- performance(pred,"tpr","fpr")
auc.perf = performance(pred, measure = "auc")
auc.perf@y.values
plot(perf)
library(gbm)
# M13a: Default Original subset
set.seed(1)
model.boo1 <-  gbm(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat +
ln_avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif,
data=data.train.std.y, distribution="gaussian", n.trees=5000,
interaction.depth=4,shrinkage=0.01,verbose=F)
pred.valid.boo1 <- predict(model.boo1, newdata = data.valid.std.y, n.trees=5000) # validation predictions
mean((y.valid - pred.valid.boo1)^2) # mean prediction error
sd((y.valid - pred.valid.boo1)^2)/sqrt(n.valid.y) # std error
library(neuralnet)
xc_vars = c("reg1","reg2","home","sr_chld","hinc","hinc2","wrat2","wrat3",
"ln_incm","ln_tgif","sr_tdon","sr_tlag")
xb_vars = c("reg1","reg2","home","chld","hinc","inca","tgif","tlag","wrat2",
"wrat3","hinc2","ln_incm","ln_tgif","ln_lgif","ln_tdon","ln_tlag",
"ln_agif","sr_chld","sr_tdon","sr_tlag")
xa_vars = c("reg1","reg2","reg3","reg4","home","chld","hinc","hinc2","genf","wrat",
"ln_avhv","incm","inca","plow","npro","tgif","lgif","rgif","tdon",
"tlag","agif")
xc1 = data.train.std.y[,xc_vars]
xb1 = data.train.std.y[,xb_vars]
xa1 = data.train.std.y[,xa_vars]
xvc1 = data.valid.std.y[,xc_vars]
xvb1 = data.valid.std.y[,xb_vars]
xva1 = data.valid.std.y[,xa_vars]
# M12a: Default Original subset
set.seed(1)
model.nn1 <-  neuralnet(donr ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + hinc2 + genf + wrat +
ln_avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif,
data=data.train.std.y, hidden=4, threshold=0.01)
newdata=xva1
pred.valid.nn1 <- predict(model.nn1, newdata = data.valid.std.y) # validation predictions
mean((y.valid - pred.valid.nn1)^2) # mean prediction error
sd((y.valid - pred.valid.nn1)^2)/sqrt(n.valid.y) # std error
set.seed(1)
model.nn1 = neuralnet(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat +
ln_avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif,
data=data.train.std.y, hidden=4, threshold=0.01)
newdata=xva
pred.valid.nn1 <- predict(model.nn1, newdata) # validation predictions
mean((y.valid - pred.valid.nn1)^2) # mean prediction error
sd((y.valid - pred.valid.nn1)^2)/sqrt(n.valid.y) # std error
# Artificial Neural Network
library(neuralnet)
xc_vars = c("reg3","reg4","reg5","home","chld","hinc","plow","wrat","wrat2",
"hinc3","ln_incm","ln_lgif","ln_tgif","ln_rgif","ln_agif")
xb_vars = c("reg3","reg4","reg5","home","chld","hinc","plow","wrat","wrat2",
"wrat3","hinc3","ln_incm","ln_plow","ln_tgif","ln_lgif","ln_rgif",
"ln_agif","sr_incm","sr_plow","sr_lgif")
xa_vars = c("reg1","reg2","reg3","reg4","home","chld","hinc","genf","wrat",
"ln_avhv","incm","inca","plow","npro","tgif","lgif","rgif","tdon",
"tlag","agif")
xc = as.matrix(data.train.std.y)[,xc_vars]
xb = as.matrix(data.train.std.y)[,xb_vars]
xa = as.matrix(data.train.std.y)[,xa_vars]
xvc = as.matrix(data.valid.std.y)[,xc_vars]
xvb = as.matrix(data.valid.std.y)[,xb_vars]
xva = as.matrix(data.valid.std.y)[,xa_vars]
xc1 = data.train.std.y[,xc_vars]
xb1 = data.train.std.y[,xb_vars]
xa1 = data.train.std.y[,xa_vars]
xvc1 = data.valid.std.y[,xc_vars]
xvb1 = data.valid.std.y[,xb_vars]
xva1 = data.valid.std.y[,xa_vars]
newdata=xva1
pred.valid.nn1 <- predict(model.nn1, newdata) # validation predictions
mean((y.valid - pred.valid.nn1)^2) # mean prediction error
sd((y.valid - pred.valid.nn1)^2)/sqrt(n.valid.y) # std error
newdata=xva1
pred.valid.nn1 <- compute(model.nn1, newdata) # validation predictions
mean((y.valid - pred.valid.nn1)^2) # mean prediction error
sd((y.valid - pred.valid.nn1)^2)/sqrt(n
newdata=xva1
pred.valid.nn1 <- compute(model.nn1, newdata) # validation predictions
mean((y.valid - pred.valid.nn1)^2) # mean prediction error
sd((y.valid - pred.valid.nn1)^2)/sqrt(n.valid.y) # std error
set.seed(1)
model.nn1 = nnet(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat +
ln_avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif,
data=data.train.std.y, size=20, maxit=100, decay=.001)
newdata=xva1
pred.valid.nn1 <- predict(model.nn1, newdata) # validation predictions
mean((y.valid - pred.valid.nn1)^2) # mean prediction error
sd((y.valid - pred.valid.nn1)^2)/sqrt(n.valid.y) # std error
library(e1071)
xc_vars = c("reg3","reg4","reg5","home","chld","hinc","plow","wrat","wrat2",
"hinc3","ln_incm","ln_lgif","ln_tgif","ln_rgif","ln_agif")
xb_vars = c("reg3","reg4","reg5","home","chld","hinc","plow","wrat","wrat2",
"wrat3","hinc3","ln_incm","ln_plow","ln_tgif","ln_lgif","ln_rgif",
"ln_agif","sr_incm","sr_plow","sr_lgif")
xa_vars = c("reg1","reg2","reg3","reg4","home","chld","hinc","genf","wrat",
"ln_avhv","incm","inca","plow","npro","tgif","lgif","rgif","tdon",
"tlag","agif")
xc = as.matrix(data.train.std.y)[,xc_vars]
xb = as.matrix(data.train.std.y)[,xb_vars]
xa = as.matrix(data.train.std.y)[,xa_vars]
xvc = as.matrix(data.valid.std.y)[,xc_vars]
xvb = as.matrix(data.valid.std.y)[,xb_vars]
xva = as.matrix(data.valid.std.y)[,xa_vars]
xc1 = data.train.std.y[,xc_vars]
xb1 = data.train.std.y[,xb_vars]
xa1 = data.train.std.y[,xa_vars]
xvc1 = data.valid.std.y[,xc_vars]
xvb1 = data.valid.std.y[,xb_vars]
xva1 = data.valid.std.y[,xa_vars]
# M13a: Default Original subset
set.seed(1)
model.nn1 = nnet(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat +
ln_avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif,
data=data.train.std.y, size=20, maxit=100, decay=.001)
newdata=xva1
pred.valid.nn1 <- predict(model.nn1, newdata) # validation predictions
mean((y.valid - pred.valid.nn1)^2) # mean prediction error
sd((y.valid - pred.valid.nn1)^2)/sqrt(n.valid.y) # std error
